{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Calculate entropy of the following distributions\n",
    "\n",
    "[0.25, 0.25, 0.25, 0.25] and [0.5, 0.3, 0.1, .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "1.68547529723\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def entropy( probabilities ):\n",
    "    return - sum( p * math.log(p, 2) for p in probabilities)\n",
    "\n",
    "print entropy( [0.25] * 4 )\n",
    "print entropy( [0.5, 0.3, 0.1, .1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Suppose that we model the consonants (C) and the vowels (V) jointly. The joint distribution is shown below. Whatâ€™s the conditional entropy H(C|V)? Compare with the entropy of the consonants and give your conclusion.\n",
    "\n",
    "\n",
    " Consonant (C)  |    Vowel (V)   |       P(C,V)\n",
    "\n",
    "    p           |    a           |        5/14\n",
    "    \n",
    "    p           |    i           |        1/14\n",
    "    \n",
    "    p           |    u           |          0\n",
    "    \n",
    "    t           |    a           |        1/14\n",
    "    \n",
    "    t           |    i           |        1/14\n",
    "    \n",
    "    t           |    u           |        1/14\n",
    "    \n",
    "    k           |    a           |        2/14\n",
    "    \n",
    "    k           |    i           |          0\n",
    "    \n",
    "    k           |    u           |        3/14    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(C) = 1.53\n",
      "H(C|V) = 1.12\n",
      "Mutual information I(C,V) = 0.41\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def conditional_entropy( joint_probs ):\n",
    "    consonants = [ 'p', 't', 'k' ]\n",
    "    vowels = ['a' , 'i' , 'u']\n",
    "    \n",
    "    marginal_vowel = defaultdict(float)\n",
    "    \n",
    "    for c in consonants:\n",
    "        for v in vowels:\n",
    "            marginal_vowel[v] += joint_probs[(c, v)]\n",
    "    \n",
    "    # Calculate P(X|Y)\n",
    "    p_x_given_y = {}\n",
    "    \n",
    "    for c in consonants:\n",
    "        for v in vowels:\n",
    "            p_x_given_y[(c,v)] = joint_probs[(c,v)] / marginal_vowel[v]\n",
    "    \n",
    "    return -sum (0 if joint_probs[(c,v)] == 0 else joint_probs[(c,v)] * math.log(p_x_given_y[(c,v)], 2) \n",
    "                 for c in consonants for v in vowels)\n",
    "\n",
    "joint_probs  = { ('p', 'a'): 5.0/14, ('p', 'i') : 1.0/14, ('p', 'u'): 0, \n",
    "               ('t', 'a'): 1.0/14, ('t', 'i'): 1.0/14, ('t', 'u'): 1.0/14,\n",
    "               ('k', 'a'): 2.0/14, ('k', 'i'): 0, ('k', 'u'): 3.0/14}\n",
    "\n",
    "ent_c = entropy( [6.0/14, 3.0/14, 5.0/14] )\n",
    "cond_ent_c_given_v = conditional_entropy(joint_probs)\n",
    "print 'H(C) = %.2f' % ent_c\n",
    "print 'H(C|V) = %.2f' % cond_ent_c_given_v\n",
    "print 'Mutual information I(C,V) = %.2f' % (ent_c - cond_ent_c_given_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Follow the instruction from Wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Shannon_coding\n",
    "\n",
    "Create Shannon encodings of the following set of symbols ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "given their corresponding distribution in our message is:\n",
    "    \n",
    "[0.36, .18, .18, .12, .09, .07]\n",
    "\n",
    "Calculate the expected length of an encoded symbol, and compare with the entropy of the symbol distribution: \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '010', '100', '1011', '1101', '1110']\n"
     ]
    }
   ],
   "source": [
    "def shannon_coding ( probs ):\n",
    "    cumulative_prob = 0\n",
    "    \n",
    "    codes = []\n",
    "    for prob in probs:\n",
    "        code_length = int(math.ceil( -math.log(prob, 2) ))\n",
    "        # First shifting the value of probability to the left by code_length \n",
    "        # in binary base\n",
    "        # zfill to fill the length of the code to required of length\n",
    "        code =  bin(int(cumulative_prob * 2 ** code_length))[2:].zfill(code_length)\n",
    "        codes.append(code)\n",
    "        cumulative_prob += prob\n",
    "        \n",
    "    return codes\n",
    "\n",
    "distribution = [0.36, .18, .18, .12, .09, .07]\n",
    "encodings = shannon_coding ( distribution )\n",
    "print encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected length of an encoded symbol = 2.92\n"
     ]
    }
   ],
   "source": [
    "print 'The expected length of an encoded symbol = %.2f' % \\\n",
    "    sum ( prob * len(encoded) for prob, encoded in zip ( distribution, encodings ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of an encoded symbol = 2.37\n"
     ]
    }
   ],
   "source": [
    "print 'The entropy of an encoded symbol = %.2f' % \\\n",
    "   entropy(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Calculate Huffman encodings for the aforementioned set of symbols.\n",
    "\n",
    "Calculate the expected length of an encoded symbol; compare with the entropy of the symbol distribution and also the expected length by Shannon encoding.\n",
    "\n",
    "This is not a mandatory exercise, but you would get extra credit for doing it. The due date is Feb 14th, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
